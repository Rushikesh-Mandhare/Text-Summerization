from flask import Flask, request, jsonify
import tensorflow as tf
import time

app = Flask(__name__)

# Load Basic Encoder-Decoder Model (for demonstration)
basic_model = tf.keras.models.load_model('models/basic_encoder_decoder.h5')

# Load Attention Encoder-Decoder Model (for demonstration)
attention_model = tf.keras.models.load_model('models/attention_encoder_decoder.h5')

# Load BERT Model (for demonstration)
bert_model = tf.keras.models.load_model('models/bert_model.h5')

# Simulate text preprocessing function
def text_preprocessing(text):
    print("Preprocessing input text...")
    time.sleep(1)  # Simulating some delay for preprocessing
    return text.strip().lower()

# Simulate model inference process 
def simulate_model_inference(model_name, processed_text):
    print(f"Simulating inference with model: {model_name}...")
    time.sleep(2)  # Simulate some time for processing
    fake_summary = f"Fake summary generated by {model_name}: {processed_text[:100]}..."
    return fake_summary

# Simulate postprocessing step 
def simulate_postprocessing(summary):
    print("Postprocessing the generated summary...")
    time.sleep(1)  # Simulate some delay for postprocessing
    return summary + " [Simulated refined output]"




@app.route('/summarize-basic', methods=['POST'])
def summarize_basic():
    text = request.json.get('text')
    
    # Step 1: Simulate preprocessing
    processed_text = text_preprocessing(text)
    
    # Step 2: Simulate inference using the Basic Encoder-Decoder model
    summary = simulate_model_inference("Basic Encoder-Decoder", processed_text)
    
    # Step 3: Simulate postprocessing on the generated summary
    final_summary = simulate_postprocessing(summary)
    
    return jsonify({"summary": final_summary})

@app.route('/summarize-attention', methods=['POST'])
def summarize_attention():
    text = request.json.get('text')
    
    # Step 1: Simulate preprocessing
    processed_text = text_preprocessing(text)
    
    # Step 2: Simulate inference using the Attention Encoder-Decoder model
    summary = simulate_model_inference("Attention Encoder-Decoder", processed_text)
    
    # Step 3: Simulate postprocessing on the generated summary
    final_summary = simulate_postprocessing(summary)
    
    return jsonify({"summary": final_summary})

@app.route('/summarize-bert', methods=['POST'])
def summarize_bert():
    text = request.json.get('text')
    
    # Step 1: Simulate preprocessing
    processed_text = text_preprocessing(text)
    
    # Step 2: Simulate inference using the BERT model
    summary = simulate_model_inference("BERT Model", processed_text)
    
    # Step 3: Simulate postprocessing on the generated summary
    final_summary = simulate_postprocessing(summary)
    
    return jsonify({"summary": final_summary})

if __name__ == '__main__':
    print("Backend server running, simulating model inference...")
    app.run(debug=True)
